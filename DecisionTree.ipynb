{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df4bc78c-3169-458b-b0a1-e542ad1fa805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import sum, count, avg, collect_set\n",
    "from typing import List, Tuple, Dict\n",
    "import math\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DecisionTree\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "157c97cd-6ef1-47a8-830b-a7b75318f954",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def transpose(cols: List[str], df: DataFrame, colgroup: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Función para rotar el dataframe\n",
    "    \n",
    "    Parameters:\n",
    "        cols(List[str]): Lsita de las columnas del DataFrame\n",
    "        df(DataFrame): Dataframe al cual se le hará transpose\n",
    "        colgroup(str): columna usada para agrupar\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Una copia del dataframe aplicando Transpose\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    columns = df.agg(collect_set(colgroup)).collect()[0][0]\n",
    "    columns.insert(0, 'temp-col')\n",
    "    for col in cols:\n",
    "        if col == colgroup:\n",
    "            continue\n",
    "        count_v = df.groupBy(colgroup, col).agg(count(\"*\")).collect()\n",
    "        for item in count_v:\n",
    "            temp_row = [f\"{col}-{item[col]}\"]\n",
    "            for i in range(1, len(columns)):\n",
    "                if columns[i] == item[colgroup]:\n",
    "                    temp_row.append(item['count(1)'])\n",
    "                else:\n",
    "                    temp_row.append(0)\n",
    "            rows.append(temp_row)\n",
    "\n",
    "    transpose_df = spark.createDataFrame(rows, columns)\n",
    "    return transpose_df\n",
    "\n",
    "\n",
    "def read_csv(path:str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Función para leer el archivo csv\n",
    "    Parameters:\n",
    "        path(str): path de ubicación del archivo csv\n",
    "    Returns:\n",
    "        List[DataFrame, List[str]]: retorna el DataFrame y la lista de las columnas\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        df = spark.read.csv(path, inferSchema=True, header=True)\n",
    "        cols = df.columns\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error leyendo el archivo')\n",
    "        print(str(e))\n",
    "    else:\n",
    "        return df, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09546326-095b-47bd-97a8-8754d9637e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_vect(df: DataFrame) -> Dict[str, str]:\n",
    "    print(\"Columnas disponibles en el dataframe:\")\n",
    "    print(df.columns)\n",
    "    \n",
    "    target_col = input(\"Selecciona la columna a predecir (target): \").strip()\n",
    "    \n",
    "    feature_cols_input = input(\"Selecciona las columnas a usar como predictores, separadas por comas: \").strip()\n",
    "    predictors = [col.strip() for col in feature_cols_input.split(\",\") if col.strip() != \"\"]\n",
    "    \n",
    "    return predictors, target_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1da84157-7e17-41bf-9ad1-dd28c3ee5179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def prepare_data(df, target_col, predictors):\n",
    "    data = df\n",
    "\n",
    "    if isinstance(df.schema[target_col].dataType, StringType):\n",
    "        si_target = StringIndexer(inputCol=target_col, outputCol=\"label\")\n",
    "        si_model_target = si_target.fit(data)\n",
    "        data = si_model_target.transform(data)\n",
    "        print(f\"\\nCodificación de '{target_col}':\")\n",
    "        for i, lbl in enumerate(si_model_target.labels):\n",
    "            print(f\"{i} -> {lbl}\")\n",
    "    else:\n",
    "        data = data.withColumnRenamed(target_col, \"label\")\n",
    "\n",
    "    feat_cols = []\n",
    "    for col_name in predictors:\n",
    "        if col_name == target_col or col_name.lower() == \"id\":\n",
    "            continue\n",
    "\n",
    "        if isinstance(df.schema[col_name].dataType, StringType):\n",
    "            si_feat = StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_idx\")\n",
    "            si_model_feat = si_feat.fit(data)\n",
    "            data = si_model_feat.transform(data)\n",
    "            feat_cols.append(f\"{col_name}_idx\")\n",
    "            print(f\"\\nCodificación de '{col_name}':\")\n",
    "            for i, lbl in enumerate(si_model_feat.labels):\n",
    "                print(f\"{i} -> {lbl}\")\n",
    "        else:\n",
    "            feat_cols.append(col_name)\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=feat_cols, outputCol=\"features\")\n",
    "    data = assembler.transform(data)\n",
    "\n",
    "    train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "    print(\"\\nMuestra de train_data:\")\n",
    "    train_data.show(5, truncate=False)\n",
    "\n",
    "    return train_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43abd01a-abd0-4a49-8d69-f468b28ceab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def fit_model(train_data, test_data):\n",
    "\n",
    "    max_depth = float(input(\"Ingresa max_depth a usar: \"))\n",
    "    min_instances = float(input(\"Ingresa min_instances a usar: \"))\n",
    "    \n",
    "    dt = DecisionTreeClassifier(\n",
    "        labelCol=\"label\",\n",
    "        featuresCol=\"features\",\n",
    "        maxDepth=max_depth,\n",
    "        minInstancesPerNode=min_instances\n",
    "    )\n",
    "\n",
    "    # Entrenar\n",
    "    model = dt.fit(train_data)\n",
    "    predictions = model.transform(test_data)\n",
    "\n",
    "    # Accuracy\n",
    "    correct = predictions.filter(col(\"prediction\") == col(\"label\")).count()\n",
    "    total = predictions.count()\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "    # Falsos positivos por clase\n",
    "    labels = [row[0] for row in predictions.select(\"label\").distinct().collect()]\n",
    "    print(\"Falsos Positivos por clase:\")\n",
    "    for l in labels:\n",
    "        fp = predictions.filter((col(\"prediction\") == l) & (col(\"label\") != l)).count()\n",
    "        print(f\"Clase {l}: {fp}\")\n",
    "\n",
    "    # arbol\n",
    "    print(\"\\nÁrbol de decisión:\")\n",
    "    print(model.toDebugString)\n",
    "\n",
    "    return model, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9401703a-dbd2-44c6-af8e-7238c3404ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "def predecir(model):\n",
    "    while True:\n",
    "        input_str = input(\"Ingresa las características separadas por comas (ej: 0.0,0.2,3.5): \")\n",
    "        if input_str == \"\":\n",
    "            break\n",
    "        \n",
    "        features = [float(x.strip()) for x in input_str.split(\",\")]\n",
    "    \n",
    "        df = spark.createDataFrame([(Vectors.dense(features),)], [\"features\"])\n",
    "    \n",
    "        prediction = model.transform(df).collect()[0][\"prediction\"]\n",
    "        print(f\"Predicción del modelo: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de45b121-68b4-4f57-964a-b0b13564f4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bienvenido al uso del modelo Decision Tree\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Por favor provee el path de ubicación del archivo csv:  iris.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Muestra del Dataframe\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|\n",
      "+---+-------------+------------+-------------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Columnas disponibles en el dataframe:\n",
      "['Id', 'SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'Species']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Selecciona la columna a predecir (target):  Species\n",
      "Selecciona las columnas a usar como predictores, separadas por comas:  PetalWidthCm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Codificación de 'Species':\n",
      "0 -> Iris-setosa\n",
      "1 -> Iris-versicolor\n",
      "2 -> Iris-virginica\n",
      "\n",
      "Muestra de train_data:\n",
      "+---+-------------+------------+-------------+------------+-----------+-----+--------+\n",
      "|Id |SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|Species    |label|features|\n",
      "+---+-------------+------------+-------------+------------+-----------+-----+--------+\n",
      "|1  |5.1          |3.5         |1.4          |0.2         |Iris-setosa|0.0  |[0.2]   |\n",
      "|2  |4.9          |3.0         |1.4          |0.2         |Iris-setosa|0.0  |[0.2]   |\n",
      "|4  |4.6          |3.1         |1.5          |0.2         |Iris-setosa|0.0  |[0.2]   |\n",
      "|5  |5.0          |3.6         |1.4          |0.2         |Iris-setosa|0.0  |[0.2]   |\n",
      "|6  |5.4          |3.9         |1.7          |0.4         |Iris-setosa|0.0  |[0.4]   |\n",
      "+---+-------------+------------+-------------+------------+-----------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ingresa max_depth a usar:  1\n",
      "Ingresa min_instances a usar:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7083333333333334\n",
      "Falsos Positivos por clase:\n",
      "Clase 0.0: 0\n",
      "Clase 1.0: 7\n",
      "Clase 2.0: 0\n",
      "\n",
      "Árbol de decisión:\n",
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_b52d9fb4969b, depth=1, numNodes=3, numClasses=3, numFeatures=1\n",
      "  If (feature 0 <= 0.8)\n",
      "   Predict: 0.0\n",
      "  Else (feature 0 > 0.8)\n",
      "   Predict: 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ingresa las características separadas por comas (ej: 0.0,0.2,3.5):  0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción del modelo: 0.0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ingresa las características separadas por comas (ej: 0.0,0.2,3.5):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción del modelo: 1.0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ingresa las características separadas por comas (ej: 0.0,0.2,3.5):  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción del modelo: 1.0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ingresa las características separadas por comas (ej: 0.0,0.2,3.5):  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción del modelo: 0.0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ingresa las características separadas por comas (ej: 0.0,0.2,3.5):  0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción del modelo: 0.0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ingresa las características separadas por comas (ej: 0.0,0.2,3.5):  3.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción del modelo: 1.0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ingresa las características separadas por comas (ej: 0.0,0.2,3.5):  \n"
     ]
    }
   ],
   "source": [
    "print(\"Bienvenido al uso del modelo Decision Tree\")\n",
    "path = input(\"Por favor provee el path de ubicación del archivo csv: \")\n",
    "df, cols = read_csv(path)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"Muestra del Dataframe\")\n",
    "df.show(5)\n",
    "\n",
    "\"\"\"while True:\n",
    "    try:\n",
    "        ans_trans = int(input(\"Desea hacer transpose al Dataframe: 1. Sí\\t2. No: \"))\n",
    "        if ans_trans == 1:\n",
    "            # Fix\n",
    "        if ans_trans == 2:\n",
    "            break\n",
    "        else:\n",
    "            print(\"Por favor, ingrese 1 para Sí o 2 para No.\")\n",
    "    except ValueError:\n",
    "        print(\"Por favor, ingrese un número válido (1 o 2).\")\"\"\"\n",
    "\n",
    "predictors, target_col = get_prediction_vect(df)\n",
    "train_data, test_data = prepare_data(df, target_col, predictors)\n",
    "\n",
    "model, predictions = fit_model(train_data, test_data)\n",
    "predecir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8792bd8f-748b-4ef5-bb7f-47e10a6e3b45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
